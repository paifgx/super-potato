{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RMSE (100k USD): 0.736\n",
            "R^2: 0.586\n",
            "RMSE (100k USD) Forest: 0.648\n",
            "R^2 Forest: 0.679\n",
            "Artifacts saved to ./model/\n"
          ]
        }
      ],
      "source": [
        "# Training and export for exactly 9 features\n",
        "# - Output target is Median House Value in 100k USD (per dataset definition)\n",
        "# - API will multiply predictions by 100_000 to report USD\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load data and build engineered features\n",
        "housing = fetch_california_housing(as_frame=True)\n",
        "df = housing.frame.rename(\n",
        "    columns={\n",
        "        \"MedInc\": \"MedInc\",\n",
        "        \"HouseAge\": \"HouseAge\",\n",
        "        \"AveRooms\": \"AveRooms\",\n",
        "        \"AveBedrms\": \"AveBedrms\",\n",
        "        \"Population\": \"Population\",\n",
        "        \"AveOccup\": \"AveOccup\",\n",
        "        \"Latitude\": \"Latitude\",\n",
        "        \"Longitude\": \"Longitude\",\n",
        "    }\n",
        ")\n",
        "\n",
        "df[\"RoomsPerPerson\"] = df[\"AveRooms\"] / df[\"AveOccup\"]\n",
        "df[\"BedrmRoomRatio\"] = df[\"AveBedrms\"] / df[\"AveRooms\"]\n",
        "df[\"LogPopulation\"] = np.log1p(df[\"Population\"])\n",
        "\n",
        "# Target (in 100k USD)\n",
        "y = housing.target\n",
        "\n",
        "# Exact 9-feature set\n",
        "features_for_model = [\n",
        "    \"MedInc\",\n",
        "    \"HouseAge\",\n",
        "    \"AveRooms\",\n",
        "    \"AveBedrms\",\n",
        "    \"Population\",\n",
        "    \"AveOccup\",\n",
        "    \"RoomsPerPerson\",\n",
        "    \"BedrmRoomRatio\",\n",
        "    \"LogPopulation\",\n",
        "]\n",
        "X = df[features_for_model].copy()\n",
        "\n",
        "# 2. Split & scale\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.20, random_state=42\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 3. Train baseline model (Linear Regression for v1)\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"RMSE (100k USD): {rmse:.3f}\")\n",
        "print(f\"R^2: {r2:.3f}\")\n",
        "\n",
        "forest_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "forest_reg.fit(X_train, y_train)\n",
        "\n",
        "y_pred_forest = forest_reg.predict(X_test)\n",
        "\n",
        "rmse_forest = mean_squared_error(y_test, y_pred_forest, squared=False)\n",
        "r2_forest = r2_score(y_test, y_pred_forest)\n",
        "\n",
        "print(f\"RMSE (100k USD) Forest: {rmse_forest:.3f}\")\n",
        "print(f\"R^2 Forest: {r2_forest:.3f}\")\n",
        "\n",
        "\n",
        "# 4. Persist artifacts\n",
        "import joblib, os, json\n",
        "os.makedirs(\"model\", exist_ok=True)\n",
        "joblib.dump(model, \"model/housing_model.pkl\")\n",
        "joblib.dump(scaler, \"model/scaler.pkl\")\n",
        "\n",
        "with open(\"model/feature_order.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(features_for_model, f)\n",
        "\n",
        "print(\"Artifacts saved to ./model/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV]  Ridge+logy  RMSE: 1.010\n",
            "[CV]  HGB         RMSE: 0.632\n",
            "[TEST]Ridge+logy  RMSE: 0.879 | R^2: 0.411\n",
            "[TEST]HGB         RMSE: 0.637 | R^2: 0.690\n",
            "\n",
            "==> Winner by CV: HGB\n",
            "\n",
            "Saved: model/housing_model.pkl, model/scaler.pkl, model/feature_order.json\n",
            "Note: Model outputs are in 100k USD. The API multiplies to USD.\n"
          ]
        }
      ],
      "source": [
        "# === BEST-OF: Ridge (log-target) vs. HistGradientBoosting ===\n",
        "# - Compares two candidates via CV-RMSE and picks the winner\n",
        "# - Evaluates on test split\n",
        "# - Persists API-compatible artifacts:\n",
        "#   * model/housing_model.pkl -> winner model\n",
        "#       - Ridge: TransformedTargetRegressor (log/inverse-log) WITHOUT scaler (expects scaled input)\n",
        "#       - HGB:   plain HistGradientBoostingRegressor (expects raw input)\n",
        "#   * model/scaler.pkl -> for API compatibility\n",
        "#       - Ridge: fitted StandardScaler\n",
        "#       - HGB:   identity FunctionTransformer (so API .transform() is a no-op)\n",
        "#   * model/feature_order.json -> exact 9 features in the order used by the API\n",
        "#\n",
        "# Unit: model predicts in 100k USD; the API multiplies to USD.\n",
        "\n",
        "import os\n",
        "import json\n",
        "import warnings\n",
        "from typing import List\n",
        "\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# 1) Data and exact 9 features aligned with the API\n",
        "housing = fetch_california_housing(as_frame=True)\n",
        "df_raw = housing.frame.copy()\n",
        "\n",
        "# Build engineered features\n",
        "df = df_raw[[\n",
        "    \"MedInc\", \"HouseAge\", \"AveRooms\", \"AveBedrms\", \"Population\", \"AveOccup\",\n",
        "]].copy()\n",
        "df[\"RoomsPerPerson\"] = df[\"AveRooms\"] / df[\"AveOccup\"]\n",
        "df[\"BedrmRoomRatio\"] = df[\"AveBedrms\"] / df[\"AveRooms\"]\n",
        "df[\"LogPopulation\"] = np.log1p(df[\"Population\"])\n",
        "\n",
        "features: List[str] = [\n",
        "    \"MedInc\",\n",
        "    \"HouseAge\",\n",
        "    \"AveRooms\",\n",
        "    \"AveBedrms\",\n",
        "    \"Population\",\n",
        "    \"AveOccup\",\n",
        "    \"RoomsPerPerson\",\n",
        "    \"BedrmRoomRatio\",\n",
        "    \"LogPopulation\",\n",
        "]\n",
        "X = df[features].copy()\n",
        "y = housing.target  # in 100k USD\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.20, random_state=42\n",
        ")\n",
        "\n",
        "# 2) Scorer & CV\n",
        "neg_rmse = \"neg_root_mean_squared_error\"\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# 3) Candidate A: Ridge with log-target\n",
        "# Train-time pipeline: StandardScaler -> TransformedTargetRegressor(Ridge)\n",
        "# We will persist ONLY the TTR (no scaler) so the API's scaler is the single scaler applied.\n",
        "ridge_ttr = TransformedTargetRegressor(\n",
        "    regressor=Ridge(), func=np.log1p, inverse_func=np.expm1\n",
        ")\n",
        "ridge_train_pipe = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"ttr\", ridge_ttr),\n",
        "])\n",
        "param_ridge = {\"ttr__regressor__alpha\": np.logspace(-3, 3, 30)}\n",
        "rs_ridge = RandomizedSearchCV(\n",
        "    ridge_train_pipe,\n",
        "    param_distributions=param_ridge,\n",
        "    n_iter=20,\n",
        "    scoring=neg_rmse,\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        ")\n",
        "rs_ridge.fit(X_train, y_train)\n",
        "\n",
        "ridge_cv_rmse = -rs_ridge.best_score_\n",
        "ridge_test_pred = rs_ridge.best_estimator_.predict(X_test)\n",
        "ridge_test_rmse = mean_squared_error(y_test, ridge_test_pred, squared=False)\n",
        "ridge_test_r2 = r2_score(y_test, ridge_test_pred)\n",
        "\n",
        "# 4) Candidate B: HistGradientBoosting (no scaler required)\n",
        "hgb = HistGradientBoostingRegressor(random_state=42)\n",
        "param_hgb = {\n",
        "    \"learning_rate\": [0.02, 0.05, 0.1],\n",
        "    \"max_depth\": [None, 4, 8, 12],\n",
        "    \"max_leaf_nodes\": [31, 63, 127],\n",
        "    \"min_samples_leaf\": [10, 20, 50, 100],\n",
        "    \"l2_regularization\": [0.0, 0.1, 1.0],\n",
        "}\n",
        "rs_hgb = RandomizedSearchCV(\n",
        "    hgb,\n",
        "    param_distributions=param_hgb,\n",
        "    n_iter=30,\n",
        "    scoring=neg_rmse,\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        ")\n",
        "rs_hgb.fit(X_train, y_train)\n",
        "\n",
        "hgb_cv_rmse = -rs_hgb.best_score_\n",
        "hgb_test_pred = rs_hgb.predict(X_test)\n",
        "hgb_test_rmse = mean_squared_error(y_test, hgb_test_pred, squared=False)\n",
        "hgb_test_r2 = r2_score(y_test, hgb_test_pred)\n",
        "\n",
        "# 5) Compare & select winner (by CV-RMSE)\n",
        "print(f\"[CV]  Ridge+logy  RMSE: {ridge_cv_rmse:,.3f}\")\n",
        "print(f\"[CV]  HGB         RMSE: {hgb_cv_rmse:,.3f}\")\n",
        "print(f\"[TEST]Ridge+logy  RMSE: {ridge_test_rmse:,.3f} | R^2: {ridge_test_r2:.3f}\")\n",
        "print(f\"[TEST]HGB         RMSE: {hgb_test_rmse:,.3f} | R^2: {hgb_test_r2:.3f}\")\n",
        "\n",
        "winner_name = \"HGB\" if hgb_cv_rmse < ridge_cv_rmse else \"Ridge+logy\"\n",
        "print(f\"\\n==> Winner by CV: {winner_name}\")\n",
        "\n",
        "# 6) Persist artifacts (API-compatible)\n",
        "os.makedirs(\"model\", exist_ok=True)\n",
        "\n",
        "if winner_name == \"Ridge+logy\":\n",
        "    # Persist only the TTR (expects scaled input), and the fitted scaler separately\n",
        "    best_pipe = rs_ridge.best_estimator_\n",
        "    model_to_save = best_pipe.named_steps[\"ttr\"]\n",
        "    scaler_to_save = best_pipe.named_steps[\"scaler\"]\n",
        "else:\n",
        "    model_to_save = rs_hgb.best_estimator_\n",
        "    scaler_to_save = FunctionTransformer(validate=False)\n",
        "    # Fit once to record feature-in meta; keep validate=False so API .transform() is a no-op\n",
        "    try:\n",
        "        scaler_to_save.fit(X_train)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "joblib.dump(model_to_save, \"model/housing_model.pkl\")\n",
        "joblib.dump(scaler_to_save, \"model/scaler.pkl\")\n",
        "\n",
        "with open(\"model/feature_order.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(features, f)\n",
        "\n",
        "print(\"\\nSaved: model/housing_model.pkl, model/scaler.pkl, model/feature_order.json\")\n",
        "print(\"Note: Model outputs are in 100k USD. The API multiplies to USD.\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
